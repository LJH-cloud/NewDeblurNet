#### general settings
name: 33_xv4k_finetune # experiment params. support debug.
use_tb_logger: true
model: SpikeDeblur
gpu_ids: [0,1]

#### description
description: "33, X4K1000FPS, finetune"

#### datasets
datasets:
  train:
    d_name: X4K1000FPS
    dataset_root: '/gpfsdata/home/lvjiahui/projects/x4k1000fps/'
    reduce_scale: 1
    length_spike: 33

    n_workers: 8   # actual n_workers
    batch_size: 44  # actual bt for both single and multi GPUs
    crop_size: 256

  val:
    d_name: X4K1000FPS
    dataset_root: '/gpfsdata/home/lvjiahui/projects/x4k1000fps/'
    reduce_scale: 1
    length_spike: 33

    n_workers: 8
    batch_size: 44
    crop_size: 0

#### path
# when set resume_state, please set pretrain_model, too
path:
  pretrain_model: '/gpfsdata/home/lvjiahui/projects/SpkDeblurNet/experiments/250617-140607_RESUME_33_xv4k_finetune/models/3000_SpikeDeblur.pth'
  strict_load: true
  resume_state: '/gpfsdata/home/lvjiahui/projects/SpkDeblurNet/experiments/250617-140607_RESUME_33_xv4k_finetune/training_state/3000_SpikeDeblur.state'

#### training settings
train:
  lr: !!float 1e-5
  # weight_decay: !!float 0
  weight_decay: !!float 1e-5
  # epsilon: !!float 1e-8
  beta1: 0.9
  beta2: 0.999

  niter: 30000
  warmup_iter: 500
  
  # lr_scheme: MultiStepLR
  lr_scheme: CosineAnnealingLR
  lr_milestones: [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000]
  # lr_milestones: [10005, 20005, 30005, 40005, 50005, 60005, 70005, 80005, 90005]
  lr_gamma: 0.5
  T_max: 30000
  # eta_min: !!float 1e-7
  eta_min: !!float 1e-6

  gradient_clipping:

  manual_seed: 1314
  val_freq: !!float 2000 #iter


#### logger
logger:
  print_freq: 100 #iter
  save_checkpoint_freq: !!float 1000 #iter